{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review=pd.read_json('/Users/Paul/Desktop/chinese_restaurants_review/review_processed.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paul\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model=word2vec.Word2Vec.load('/Users/Paul/Desktop/chinese_restaurants_review/w2v_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52357807460004846"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.similarity('man', 'boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300   # Word vector dimensionality                      \n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a set of words we have learned in model\n",
    "\n",
    "word_set = set(w2v_model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Average(token):\n",
    "    \n",
    "    # Here 300 is the number of neurals in the hidden layer\n",
    "    feature_vec = np.zeros(num_features)\n",
    "    \n",
    "    n_words = 0\n",
    "    # we have trained a word2vec model named 'w2v_model'     \n",
    "   \n",
    "    \n",
    "    # arverage the word vectors\n",
    "    for x in token:\n",
    "        if x in word_set:\n",
    "            n_words+= 1\n",
    "            feature_vec += w2v_model[x]\n",
    "    \n",
    "    return np.divide(feature_vec, n_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spent: 209.3006944656372\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "begin = time.time()\n",
    "review['w2v_feature'] =review['token'].apply(lambda x: Average(x))\n",
    "print ('Total time spent:', time.time() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix = []\n",
    "for i in range(review.shape[0]):\n",
    "    if np.any(np.isnan(review.w2v_feature.iloc[i])): \n",
    "        ix.append(i)\n",
    "\n",
    "review = review.drop(ix, axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t=review['w2v_feature'].apply(lambda x: x.reshape(1, num_features)) # convert each item in 'w2v_feature' column into a numpy array\n",
    "X=np.array([x for x in review['w2v_feature']]) # convert the pandas series review['w2v_feature'] to a numpy array\n",
    "X=X.T  # X numpy array of shape (num_features, number of examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalization of the input features\n",
    "\n",
    "m=X.shape[1] # m: the number of vectors\n",
    "X_average=np.sum(X, axis=1, keepdims=True)/m\n",
    "\n",
    "# make the average of X to be zero vector (approximately)\n",
    "X=np.subtract(X, X_average)\n",
    "\n",
    "# make the standard deviation of each feature to be 1\n",
    "\n",
    "S=X*X\n",
    "std_dev_square=np.sum(S, axis=1, keepdims=True)/m\n",
    "X=np.divide(X, np.sqrt(std_dev_square))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/Users/Paul/Desktop/chinese_restaurants_review/reviews_vectors', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.load('/Users/Paul/Desktop/chinese_restaurants_review/reviews_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform stars to decimals between 0 and 1.\n",
    "\n",
    "Y=review['stars'].values.reshape(1, -1)\n",
    "Y=(Y-1)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 156607)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/Users/Paul/Desktop/chinese_restaurants_review/rating', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y=np.load('/Users/Paul/Desktop/chinese_restaurants_review/rating.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spent: 4.611070871353149\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(X.T, Y.T, test_size = 0.05, random_state = 1024)\n",
    "print ('Total time spent:', time.time() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148776, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adjust the shapes of data\n",
    "\n",
    "train_x=train_x.T\n",
    "test_x=test_x.T\n",
    "train_y=train_y.T\n",
    "test_y=test_y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('/Users/Paul/Desktop/chinese_restaurants_review/training_test_data.npz', train_x=train_x, test_x=test_x, train_y=train_y, test_y=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=np.load('/Users/Paul/Desktop/chinese_restaurants_review/training_test_data.npz')\n",
    "train_x, test_x, train_y, test_y=data['train_x'], data['test_x'], data['train_y'], data['test_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  \n",
    "    s = 1/(1+np.exp(-x))\n",
    "        \n",
    "    return s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \n",
    "    s=x*(x>0)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize parameters\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "       \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # L-1=number of layers in the NN. Note that the first item of layer_dims is the number of features.\n",
    "\n",
    "    # He initialization\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*np.sqrt(2.0/layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "                \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \n",
    "    Z = np.dot(W, A)+b    \n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data), with size of previous layer, number of examples\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        \n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache= sigmoid(Z), Z\n",
    "        \n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        \n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z), Z\n",
    "       \n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "       \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)],  'sigmoid')\n",
    "    caches.append(cache)\n",
    "   \n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute Cost function\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to the label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example:  0.2 if 1 star is given, 1 if 5 star is given), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "       \n",
    "    cost = -(np.dot(Y, np.log(AL).T)+np.dot(1-Y, np.log(1-AL).T))/m\n",
    "        \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    \n",
    "    dW = (1/m)*np.dot(dZ, A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev =np.dot(W.T, dZ)\n",
    "    \n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#linear activation backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"   \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dZ = dA*(activation_cache>0)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "       \n",
    "        dZ = dA*(sigmoid(activation_cache)*(1-sigmoid(activation_cache)))\n",
    "        dA_prev, dW, db =linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"  \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = -(np.divide(Y, AL)-np.divide(1-Y, 1-AL))/m\n",
    "   \n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')\n",
    "  \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads['dA'+str(l+2)], current_cache, 'relu')\n",
    "        grads[\"dA\" + str(l + 1)] =  dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] =  dW_temp\n",
    "        grads[\"db\" + str(l + 1)] =  db_temp\n",
    "       \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Update parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters \n",
    "    grads -- python dictionary containing the gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing the updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters['W'+str(l+1)]-=learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters['b'+str(l+1)]-=learning_rate*grads['db'+str(l+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers_dims = [num_features, 20, 20, 10, 1] #  Set up a 4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.003, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "        \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of features, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "      \n",
    "    \n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    " \n",
    "        \n",
    "        update_parameters(parameters, grads, learning_rate)\n",
    "       \n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.023045\n",
      "Cost after iteration 100: 0.481348\n",
      "Cost after iteration 200: 0.475922\n",
      "Cost after iteration 300: 0.473222\n",
      "Cost after iteration 400: 0.471492\n",
      "Cost after iteration 500: 0.470242\n",
      "Cost after iteration 600: 0.469227\n",
      "Cost after iteration 700: 0.468367\n",
      "Cost after iteration 800: 0.467607\n",
      "Cost after iteration 900: 0.466919\n",
      "Cost after iteration 1000: 0.466366\n",
      "Cost after iteration 1100: 0.466053\n",
      "Cost after iteration 1200: 0.465508\n",
      "Cost after iteration 1300: 0.465212\n",
      "Cost after iteration 1400: 0.465146\n",
      "Cost after iteration 1500: 0.464673\n",
      "Cost after iteration 1600: 0.464273\n",
      "Cost after iteration 1700: 0.464163\n",
      "Cost after iteration 1800: 0.463945\n",
      "Cost after iteration 1900: 0.463439\n",
      "Cost after iteration 2000: 0.463073\n",
      "Cost after iteration 2100: 0.463026\n",
      "Cost after iteration 2200: 0.463050\n",
      "Cost after iteration 2300: 0.462535\n",
      "Cost after iteration 2400: 0.462077\n",
      "Cost after iteration 2500: 0.461784\n",
      "Cost after iteration 2600: 0.461456\n",
      "Cost after iteration 2700: 0.461284\n",
      "Cost after iteration 2800: 0.461113\n",
      "Cost after iteration 2900: 0.460868\n",
      "Cost after iteration 3000: 0.460956\n",
      "Cost after iteration 3100: 0.460864\n",
      "Cost after iteration 3200: 0.461171\n",
      "Cost after iteration 3300: 0.461367\n",
      "Cost after iteration 3400: 0.460576\n",
      "Cost after iteration 3500: 0.459846\n",
      "Cost after iteration 3600: 0.459904\n",
      "Cost after iteration 3700: 0.460098\n",
      "Cost after iteration 3800: 0.460398\n",
      "Cost after iteration 3900: 0.460786\n",
      "Cost after iteration 4000: 0.459347\n",
      "Cost after iteration 4100: 0.458868\n",
      "Cost after iteration 4200: 0.459091\n",
      "Cost after iteration 4300: 0.460197\n",
      "Cost after iteration 4400: 0.458879\n",
      "Cost after iteration 4500: 0.459249\n",
      "Cost after iteration 4600: 0.458352\n",
      "Cost after iteration 4700: 0.458526\n",
      "Cost after iteration 4800: 0.459731\n",
      "Cost after iteration 4900: 0.458084\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9TgUzMys578AoUv4FAOOhEEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28c77615470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spent: 7897.139560222626\n"
     ]
    }
   ],
   "source": [
    "begin=time.time()\n",
    "\n",
    "parameters = L_layer_model(train_x, train_y, layers_dims, learning_rate = 162000, num_iterations =5000, print_cost = True)\n",
    "\n",
    "print ('Total time spent:', time.time() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    With the learned parameters, predicts rating for each column vector in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- learned parameters, python dictionary  \n",
    "    X -- input development/test data \n",
    "    \n",
    "    Returns\n",
    "    predictions -- numpy array of values corresponding to postive integers 1, 2, ... 5\n",
    "    \"\"\"\n",
    "        \n",
    "    AL, caches =L_model_forward(X, parameters)\n",
    "    \n",
    "    predictions = np.zeros((1, X.shape[1]))\n",
    "    \n",
    "    # the values are assigned by looking at closest value to the probability\n",
    "    \n",
    "    for i in range(AL.shape[1]):\n",
    "        if AL[0][i]>0.875:\n",
    "            predictions[0][i]=1.0\n",
    "        elif AL[0][i]>0.625:\n",
    "            predictions[0][i]=0.75\n",
    "        elif AL[0][i]>0.375:\n",
    "            predictions[0][i]=0.5\n",
    "        elif AL[0][i]>0.125:\n",
    "            predictions[0][i]=0.25           \n",
    "        else:\n",
    "            predictions[0][i]=0      \n",
    "    return predictions, AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56212488826458951"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test accuracy\n",
    "predictions, AL=predict(parameters, test_x)\n",
    "\n",
    "np.sum(predictions==test_y)/(test_y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/Users/Paul/Desktop/chinese_restaurants_review/parameters.npy', parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
