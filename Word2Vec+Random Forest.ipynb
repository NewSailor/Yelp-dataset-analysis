{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review=pd.read_json('/Users/Paul/Desktop/chinese_restaurants_review/review_processed.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>token</th>\n",
       "      <th>useful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[place, gem, friendly, attentive, service, foo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>[perhaps, closest, pho, restaurant, port, cred...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[happened, stumble, upon, little, quaint, rest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                              token  useful\n",
       "0      5  [place, gem, friendly, attentive, service, foo...       0\n",
       "1      5  [perhaps, closest, pho, restaurant, port, cred...       0\n",
       "2      4  [happened, stumble, upon, little, quaint, rest...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paul\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300   # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10         # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spent: 193.356201171875\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "w2v_model = word2vec.Word2Vec(review.token, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "print ('Total time spent:', time.time() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model=word2vec.Word2Vec.load('/Users/Paul/Desktop/chinese_restaurants_review/w2v_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51868450256167309"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.similarity('good', 'awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Average(token):\n",
    "    \n",
    "    # Here 300 is the number of neurals in the hidden layer\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    \n",
    "    n_words = 0\n",
    "    # we have trained a word2vec model named 'w2v_model'\n",
    "    \n",
    "    # make a set of words we have learned in model\n",
    "    word_set = set(w2v_model.wv.index2word)\n",
    "    \n",
    "    # only arverage the words that has certain correlation/similarity with the judgement words \"good\" or \"bad\". The others are ignored.\n",
    "    for x in token:\n",
    "        if x in word_set and (w2v_model.similarity('good', x)>=0 or w2v_model.similarity('bad', x)>=0):\n",
    "            n_words+= 1\n",
    "            feature_vec += w2v_model[x]\n",
    "    \n",
    "    return np.divide(feature_vec,float(n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spent: 1610.818256855011\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "begin = time.time()\n",
    "review['w2v_feature'] =review['token'].apply(lambda x: Average(x))\n",
    "review['w2v_feature']=review['w2v_feature']*(review['useful']+1)\n",
    "print ('Total time spent:', time.time() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>token</th>\n",
       "      <th>useful</th>\n",
       "      <th>w2v_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[place, gem, friendly, attentive, service, foo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.198498, 0.120218, -0.0277819, 0.185768, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>[perhaps, closest, pho, restaurant, port, cred...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0406628, 0.392262, -0.204646, 0.13495, 0.44...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[happened, stumble, upon, little, quaint, rest...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.225141, 0.415271, -0.686797, 1.06412, 0.66...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                              token  useful  \\\n",
       "0      5  [place, gem, friendly, attentive, service, foo...       0   \n",
       "1      5  [perhaps, closest, pho, restaurant, port, cred...       0   \n",
       "2      4  [happened, stumble, upon, little, quaint, rest...       1   \n",
       "\n",
       "                                         w2v_feature  \n",
       "0  [0.198498, 0.120218, -0.0277819, 0.185768, 0.6...  \n",
       "1  [0.0406628, 0.392262, -0.204646, 0.13495, 0.44...  \n",
       "2  [-0.225141, 0.415271, -0.686797, 1.06412, 0.66...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix = []\n",
    "for i in range(review.shape[0]):\n",
    "    if np.any(np.isnan(review.w2v_feature.iloc[i])): ix.append(i)\n",
    "review = review.drop(ix, axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "feature = np.array([x for x in review.w2v_feature])\n",
    "X_train,  X_test, y_train, y_test = train_test_split(feature, review['stars'], test_size = 0.2, random_state = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, max_depth=15)\n",
    "model_w2v= forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict=model_w2v.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53094264912504785"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test accuracy\n",
    "np.sum(predict==y_test)/(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951664601051327"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([(predict>=3) & (y_test>=3)])/(np.sum(y_test>=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review.to_json('/Users/Paul/Desktop/chinese_restaurants_review/review_w2v.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review=pd.read_json('/Users/Paul/Desktop/chinese_restaurants_review/review_w2v.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_size = 300\n",
    "n_node_hl1 = 100\n",
    "n_node_hl2 = 100\n",
    "n_class = 5\n",
    "batch_size = 200\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholers\n",
    "x = tf.placeholder('float', [None, feature_size])\n",
    "\n",
    "# should assign shape?\n",
    "y = tf.placeholder('float', [None, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_model(data):\n",
    "    # Weights and bias of each layer\n",
    "    \n",
    "    hl1 = {'weights': tf.Variable(tf.random_normal([feature_size, n_node_hl1])),\n",
    "            'bias': tf.Variable(tf.random_normal([n_node_hl1])) }\n",
    "    hl2 = {'weights': tf.Variable(tf.random_normal([n_node_hl1, n_node_hl2])),\n",
    "            'bias': tf.Variable(tf.random_normal([n_node_hl2])) }\n",
    "\n",
    "    output_layer = { 'weights': tf.Variable(tf.random_normal([n_node_hl2, n_class])),\n",
    "                'bias' : tf.Variable(tf.random_normal([n_class]))}\n",
    "    \n",
    "    # Output of each layer\n",
    "    # Relu((x*W + bias))\n",
    "    s1 = tf.add(tf.matmul(data, hl1['weights']),hl1['bias'])\n",
    "    a1 = tf.nn.relu(s1)\n",
    "    \n",
    "    s2 = tf.add(tf.matmul(a1, hl2['weights']), hl2['bias'])\n",
    "    a2 = tf.nn.relu(s2)\n",
    "    \n",
    "    output = tf.add(tf.matmul(a2, output_layer['weights']), output_layer['bias'])\n",
    "    \n",
    "    #return tf.reshape(tf.cast(tf.argmax(output,1),'float'), [batch_size, 1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NN_model(x,y, n_epoch):\n",
    "    prediction = NN_model(x)\n",
    "    cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits = prediction, labels=y))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # cut into batches\n",
    "        i = 0\n",
    "        while i < len(X_train):\n",
    "            start = i\n",
    "            end = i + batch_size\n",
    "            batch_X = X_train[start: end]\n",
    "            \n",
    "            # the original y is single numeric labels, we transform into one hot \n",
    "            # but the out put of tf.one_hot is a tensor node, so we need to sess.run(batch_y)\n",
    "            batch_y = \\\n",
    "            tf.one_hot(np.array(y_train[start: end]-1),5,on_value=1.0, off_value=0.0)\n",
    "        \n",
    "            _, c = sess.run([optimizer, cost],feed_dict={x: batch_X, y:sess.run(batch_y)})\n",
    "            \n",
    "            epoch_loss += c\n",
    "            \n",
    "            i += batch_size\n",
    "        print  ('Epoch:', epoch, 'loss:', epoch_loss)\n",
    "    \n",
    "    \n",
    "    correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    \n",
    "    one_hot_y_test = tf.one_hot(np.array(y_test-1),5,on_value=1.0, off_value=0.0)\n",
    "    print ('Accuracy', accuracy.eval(feed_dict={x: X_test, y: sess.run(one_hot_y_test)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 6776505.1095\n",
      "Epoch: 1 loss: 2184694.57111\n",
      "Epoch: 2 loss: 1029324.89551\n",
      "Epoch: 3 loss: 497760.010345\n",
      "Epoch: 4 loss: 303728.63221\n",
      "Epoch: 5 loss: 232924.048691\n",
      "Epoch: 6 loss: 199989.409782\n",
      "Epoch: 7 loss: 182444.223709\n",
      "Epoch: 8 loss: 170315.700058\n",
      "Epoch: 9 loss: 162480.446579\n",
      "Accuracy 0.446481\n",
      "Total time spent: 8609.180506944656\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "train_NN_model(x,y,n_epoch)\n",
    "print ('Total time spent:', time.time() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
